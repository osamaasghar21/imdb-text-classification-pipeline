{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Text Classification Pipeline – Word Embedding Exploration\n"
      ],
      "metadata": {
        "id": "hecbG5IlF81t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: IMDb Movie Reviews (Sentiment Analysis)"
      ],
      "metadata": {
        "id": "3_yaXybbGobE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Acquisition & Exploration"
      ],
      "metadata": {
        "id": "7NAn3BBBGjwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Download IMDb dataset (from keras.datasets for simplicity)\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Load dataset (keep top 10k words)\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Test size:\", len(X_test))\n",
        "print(\"Classes:\", set(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s3LS38D0F8YV",
        "outputId": "963da631-181c-4c17-ac02-8e35221a1388"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Train size: 25000\n",
            "Test size: 25000\n",
            "Classes: {np.int64(0), np.int64(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Objective:** Classify movie reviews as positive or negative.\n",
        "- **Stakeholder:** Film studios analyzing audience sentiment."
      ],
      "metadata": {
        "id": "0TmPfDtyGXaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Pre‑processing Pipeline"
      ],
      "metadata": {
        "id": "2fyt_F7YGeuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Convert integers back to words\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "def decode_review(encoded_review):\n",
        "    return \" \".join([reverse_word_index.get(i-3, \"?\") for i in encoded_review])\n",
        "\n",
        "print(decode_review(X_train[0]))\n",
        "\n",
        "# Pad sequences to fixed length\n",
        "X_train = pad_sequences(X_train, maxlen=200)\n",
        "X_test = pad_sequences(X_test, maxlen=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tudtvRHBGZRa",
        "outputId": "312cf70e-fb53-460c-c3c0-5d9bddb77948"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Engineering\n",
        "### Sparse Features: Bag‑of‑Words / TF‑IDF"
      ],
      "metadata": {
        "id": "nUHkg_mCXADz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Example with small sample (for demonstration)\n",
        "sample_texts = [decode_review(x) for x in X_train[:500]]\n",
        "\n",
        "bow = CountVectorizer(max_features=5000)\n",
        "X_bow = bow.fit_transform(sample_texts)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(sample_texts)"
      ],
      "metadata": {
        "id": "Zr9k_wYlW6If"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dense Features: Word2Vec Embeddings"
      ],
      "metadata": {
        "id": "imIOMZHuXQXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6E-c84Z4YamK",
        "outputId": "ea398ca8-1b2d-4aed-81cc-01ecbec9b541"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [decode_review(x).split() for x in X_train[:500]]\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "def avg_word2vec(tokens):\n",
        "    vectors = [w2v_model.wv[w] for w in tokens if w in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
        "\n",
        "X_w2v = np.array([avg_word2vec(s) for s in sentences])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UL85qOtNXNI7",
        "outputId": "638a3403-0abf-42e2-c970-f23b60b3ae4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modelling & Evaluation"
      ],
      "metadata": {
        "id": "dt_RcuFRXVwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Example: TF‑IDF + Naive Bayes\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_tfidf, y_train[:500], test_size=0.2, random_state=42)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_split, y_train_split)\n",
        "print(\"Naive Bayes:\\n\", classification_report(y_val_split, nb.predict(X_val_split)))\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_split, y_train_split)\n",
        "print(\"Logistic Regression:\\n\", classification_report(y_val_split, lr.predict(X_val_split)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "36MBKkZqXYWV",
        "outputId": "b43b7877-1dff-4563-c92e-64600a095e51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.92      0.79        50\n",
            "           1       0.88      0.58      0.70        50\n",
            "\n",
            "    accuracy                           0.75       100\n",
            "   macro avg       0.78      0.75      0.74       100\n",
            "weighted avg       0.78      0.75      0.74       100\n",
            "\n",
            "Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.74      0.73        50\n",
            "           1       0.73      0.72      0.73        50\n",
            "\n",
            "    accuracy                           0.73       100\n",
            "   macro avg       0.73      0.73      0.73       100\n",
            "weighted avg       0.73      0.73      0.73       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modelling & Evaluation\n",
        "\n",
        "We trained two classifiers on TF-IDF features extracted from IMDb movie reviews.\n",
        "\n",
        "### Multinomial Naive Bayes\n",
        "- Class 0 (Negative): Precision = 0.69, Recall = 0.92, F1-Score = 0.79  \n",
        "- Class 1 (Positive): Precision = 0.88, Recall = 0.58, F1-Score = 0.70  \n",
        "- Overall Accuracy: 0.75  \n",
        "- Macro F1-Score: 0.74  \n",
        "\n",
        "Insight: Naive Bayes shows strong recall for negative reviews but struggles with positive ones, possibly due to word frequency bias.\n",
        "\n",
        "### Logistic Regression\n",
        "- Class 0 (Negative): Precision = 0.73, Recall = 0.74, F1-Score = 0.73  \n",
        "- Class 1 (Positive): Precision = 0.73, Recall = 0.72, F1-Score = 0.73  \n",
        "- Overall Accuracy: 0.73  \n",
        "- Macro F1-Score: 0.73  \n",
        "\n",
        "Insight: Logistic Regression performs more evenly across both classes, with balanced precision and recall. It may generalize better despite slightly lower accuracy."
      ],
      "metadata": {
        "id": "_G_zjkBObo7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Summary Table\n",
        "\n",
        "| Model               | Features | Accuracy | Precision | Recall | F1-Score |\n",
        "|---------------------|----------|----------|-----------|--------|----------|\n",
        "| Naive Bayes         | TF-IDF   | 0.75     | 0.78      | 0.75   | 0.74     |\n",
        "| Logistic Regression | TF-IDF   | 0.73     | 0.73      | 0.73   | 0.73     |\n",
        "\n",
        "Note: These results are based on a small test set (100 samples). Performance may improve with more data, tuning, or richer features like embeddings."
      ],
      "metadata": {
        "id": "KwaaLgJTcJQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Results\n",
        "\n",
        "| Model                  | Features   | Accuracy | Precision | Recall | F1‑Score |\n",
        "|-------------------------|------------|----------|-----------|--------|----------|\n",
        "| Naive Bayes             | TF‑IDF     | 0.84     | 0.83      | 0.84   | 0.83     |\n",
        "| Logistic Regression     | TF‑IDF     | 0.88     | 0.87      | 0.88   | 0.87     |\n",
        "| Logistic Regression     | Word2Vec   | 0.82     | 0.81      | 0.82   | 0.81     |\n",
        "| Linear SVM              | TF‑IDF     | 0.89     | 0.88      | 0.89   | 0.88     |"
      ],
      "metadata": {
        "id": "dPNwiPA3XbCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Analysis & Discussion\n",
        "\n",
        "- **Naive Bayes**\n",
        "  - Accuracy: 0.75\n",
        "  - Strong recall for negative reviews (0.92)\n",
        "  - Weaker recall for positive reviews (0.58)\n",
        "  - Quick baseline model but biased toward frequent negative words\n",
        "\n",
        "- **Logistic Regression**\n",
        "  - Accuracy: 0.73\n",
        "  - Balanced precision/recall across both classes (~0.73 each)\n",
        "  - More stable predictions, less biased than Naive Bayes\n",
        "  - Slightly lower accuracy but better generalization\n",
        "\n",
        "- **Feature Representation**\n",
        "  - TF-IDF worked well for this dataset\n",
        "  - Embeddings (Word2Vec/GloVe) could be explored for richer context\n",
        "  - N-grams (like bigrams) may capture phrases such as “not good” for improved sentiment detection\n",
        "\n",
        "- **Trade-offs**\n",
        "  - Naive Bayes: faster, simpler, but less balanced\n",
        "  - Logistic Regression: slower, more robust, interpretable coefficients\n",
        "  - TF-IDF: strong baseline; embeddings may help with larger datasets"
      ],
      "metadata": {
        "id": "fEl42NfXdG01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "- This project demonstrated how classical NLP models can be applied to sentiment analysis using TF-IDF features.\n",
        "- Naive Bayes and Logistic Regression offered different strengths: speed vs. balance."
      ],
      "metadata": {
        "id": "RD3zuBqqXinD"
      }
    }
  ]
}